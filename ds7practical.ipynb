{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29881d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\solan\\anaconda1\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\solan\\anaconda1\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\solan\\anaconda1\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\solan\\anaconda1\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\solan\\anaconda1\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\solan\\anaconda1\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\solan\\anaconda1\\lib\\site-packages\\diamondpriceprediction-0.0.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\solan\\anaconda1\\lib\\site-packages\\fonttools-4.47.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83431902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c5bb1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\solan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\solan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\solan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\solan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download resources if needed\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('stopwords')  # For stop words\n",
    "nltk.download('wordnet')  # For lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93c11967",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics and computer science. It involves analyzing and understanding text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f155c19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', 'and', 'computer', 'science', '.', 'It', 'involves', 'analyzing', 'and', 'understanding', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf312e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('that', 'WDT'), ('combines', 'VBZ'), ('linguistics', 'NNS'), ('and', 'CC'), ('computer', 'NN'), ('science', 'NN'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('analyzing', 'VBG'), ('and', 'CC'), ('understanding', 'JJ'), ('text', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cadd7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stop Words Removal: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science', '.', 'involves', 'analyzing', 'understanding', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Tokens after Stop Words Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74b625b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stemming: ['natur', 'languag', 'process', '(', 'nlp', ')', 'fascin', 'field', 'combin', 'linguist', 'comput', 'scienc', '.', 'involv', 'analyz', 'understand', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Tokens after Stemming:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1795fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Lemmatization: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'combine', 'linguistics', 'computer', 'science', '.', 'involves', 'analyzing', 'understanding', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Tokens after Lemmatization:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4de1282f",
   "metadata": {},
   "source": [
    "Explanation of Preprocessing Steps\n",
    "Tokenization -- Splits the document into individual words or tokens.\n",
    "POS Tagging -- Assigns part-of-speech tags to each token.\n",
    "Stop Words Removal -- Removes common words (like \"the\", \"is\", \"in\") that do not add much value to analysis.\n",
    "Stemming -- Reduces words to their root form (e.g., \"processing\" becomes \"process\").\n",
    "Lemmatization -- Reduces words to their base form, considering context and part-of-speech (e.g., \"understanding\" becomes \"understand\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4653d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bc31121",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Natural Language Processing (NLP) is a fascinating field.\",\n",
    "    \"It combines linguistics and computer science.\",\n",
    "    \"NLP involves analyzing and understanding text.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5ca3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c76dba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the documents into a TF-IDF representation\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b54f740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33ffcec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TF-IDF matrix to a DataFrame for easier visualization\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59518f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation of Documents:\n",
      "   analyzing       and  combines  computer  fascinating     field  involves  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000     0.389888  0.389888  0.000000   \n",
      "1   0.000000  0.322002  0.423394  0.423394     0.000000  0.000000  0.000000   \n",
      "2   0.440362  0.334907  0.000000  0.000000     0.000000  0.000000  0.440362   \n",
      "\n",
      "         is        it  language  linguistics   natural       nlp  processing  \\\n",
      "0  0.389888  0.000000  0.389888     0.000000  0.389888  0.296520    0.389888   \n",
      "1  0.000000  0.423394  0.000000     0.423394  0.000000  0.000000    0.000000   \n",
      "2  0.000000  0.000000  0.000000     0.000000  0.000000  0.334907    0.000000   \n",
      "\n",
      "    science      text  understanding  \n",
      "0  0.000000  0.000000       0.000000  \n",
      "1  0.423394  0.000000       0.000000  \n",
      "2  0.000000  0.440362       0.440362  \n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Representation of Documents:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acaad171",
   "metadata": {},
   "source": [
    "Explanation of TF-IDF\n",
    "TF-IDF Vectorizer: This transforms a collection of text documents into a TF-IDF representation.\n",
    "TF-IDF Matrix: This matrix contains the TF-IDF scores for each term in each document.\n",
    "Feature Names: The terms used in the TF-IDF representation.\n",
    "DataFrame Representation: Converting the TF-IDF matrix into a DataFrame makes it easier to view and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685a4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
